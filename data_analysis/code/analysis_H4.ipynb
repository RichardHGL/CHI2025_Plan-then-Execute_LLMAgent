{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 complete the NASA-TLX\n",
      "248 complete the whole study\n",
      "{'UP-AE': 61, 'AP-AE': 63, 'AP-UE': 64, 'UP-UE': 60}\n"
     ]
    }
   ],
   "source": [
    "from util import load_user_data, task_order\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "valid_users, tp_data = load_user_data(folder_name=\"../anonymized_data\", reserved_users=None)\n",
    "user2condition = tp_data['user2condition']\n",
    "condition_count = {}\n",
    "for user in valid_users:\n",
    "    tp_condition = user2condition[user]\n",
    "    if tp_condition not in condition_count:\n",
    "        condition_count[tp_condition] = 0\n",
    "    condition_count[tp_condition] += 1\n",
    "print(condition_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict = {}\n",
    "original_quality = {\n",
    "    'test-149': 2,\n",
    "    'test-200': 3,\n",
    "    'test-859': 3,\n",
    "    'test-388': 5,\n",
    "    'test-497': 5,\n",
    "    'test-675': 5\n",
    "}\n",
    "all_conditions = [\"AP-AE\", \"AP-UE\", \"UP-AE\", \"UP-UE\"]\n",
    "condition_dict = {}\n",
    "for condition in all_conditions:\n",
    "    condition_dict[condition] = {}\n",
    "    for task_id in task_order + [\"avg\"]:\n",
    "        condition_dict[condition][task_id] = {\n",
    "            \"acc_strict\": [],\n",
    "            \"acc_execution\": []\n",
    "        }\n",
    "\n",
    "plan_quality = tp_data['plan_quality']\n",
    "task_performance = tp_data[\"task_performance\"]\n",
    "for task_id in task_order + [\"avg\"]:\n",
    "    variable_dict[task_id] = {\n",
    "        \"condition\": [],\n",
    "        \"planning\": [],\n",
    "        \"execution\": [],\n",
    "        \"acc_execution\": [],\n",
    "        \"acc_strict\": []\n",
    "    }\n",
    "\n",
    "for user in valid_users:\n",
    "    tp_condition = user2condition[user]\n",
    "    tp_list = {\n",
    "        \"acc_strict\": [],\n",
    "        \"acc_execution\": []\n",
    "    }\n",
    "    for task_id in task_order:\n",
    "        tp_plan_quality = plan_quality[user][task_id]\n",
    "        # skip tasks where plan quality worse than the original quality\n",
    "        if tp_plan_quality < original_quality[task_id]:\n",
    "            continue\n",
    "        for dimension in [\"acc_strict\", \"acc_execution\"]:\n",
    "            tp_val = task_performance[user][task_id][dimension]\n",
    "            variable_dict[task_id][dimension].append(tp_val)\n",
    "            tp_list[dimension].append(tp_val)\n",
    "            condition_dict[tp_condition][task_id][dimension].append(tp_val)\n",
    "        variable_dict[task_id][\"condition\"].append(tp_condition)\n",
    "        if tp_condition.startswith(\"AP\"):\n",
    "            variable_dict[task_id][\"planning\"].append(\"automatic\")\n",
    "        else:\n",
    "            variable_dict[task_id][\"planning\"].append(\"user-involved\")\n",
    "        if tp_condition.endswith(\"AE\"):\n",
    "            variable_dict[task_id][\"execution\"].append(\"automatic\")\n",
    "        else:\n",
    "            variable_dict[task_id][\"execution\"].append(\"user-involved\")\n",
    "    # print(user, tp_condition, len(tp_list))\n",
    "    if len(tp_list[\"acc_execution\"]) == 0:\n",
    "        print(f\"User {user} has no task where plan quality not decrease\")\n",
    "        assert False\n",
    "    else:\n",
    "        for dimension in [\"acc_strict\", \"acc_execution\"]:\n",
    "            tp_val = np.mean(tp_list[dimension])\n",
    "            variable_dict[\"avg\"][dimension].append(tp_val)\n",
    "            condition_dict[tp_condition][\"avg\"][dimension].append(tp_val)\n",
    "        variable_dict[\"avg\"][\"condition\"].append(tp_condition)\n",
    "        if tp_condition.startswith(\"AP\"):\n",
    "            variable_dict[\"avg\"][\"planning\"].append(\"automatic\")\n",
    "        else:\n",
    "            variable_dict[\"avg\"][\"planning\"].append(\"user-involved\")\n",
    "        if tp_condition.endswith(\"AE\"):\n",
    "            variable_dict[\"avg\"][\"execution\"].append(\"automatic\")\n",
    "        else:\n",
    "            variable_dict[\"avg\"][\"execution\"].append(\"user-involved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "\n",
    "def post_hoc_comparison(data_list_1, data_list_2, name1, name2):\n",
    "\t# print(\"Use pots-hoc analysis\")\n",
    "\tthreshold = 0.05 / 4\n",
    "\tflag = False\n",
    "\tstatistic, pvalue = mannwhitneyu(data_list_1, data_list_2, alternative='greater')\n",
    "\tif pvalue < threshold:\n",
    "\t\tprint(\"Alternative {} > {},\".format(name1, name2), \"pvalue %.4f\"%pvalue, \"statistic %.4f\"%statistic)\n",
    "\t\tflag = True\n",
    "\tstatistic, pvalue = mannwhitneyu(data_list_1, data_list_2, alternative='less')\n",
    "\tif pvalue < threshold:\n",
    "\t\tprint(\"Alternative {} < {},\".format(name1, name2), \"pvalue %.4f\"%pvalue, \"statistic %.4f\"%statistic)\n",
    "\t\tflag = True\n",
    "\tif not flag:\n",
    "\t\t# print(\"No significant difference with post-hoc analysis\")\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_strict\n",
      "test-149\n",
      "122 121\n",
      "AE vs UE; kruskal test result: H:0.09, p:0.764\n",
      "AE performance: 0.05\n",
      "UE performance: 0.06\n",
      "----------------\n",
      "test-200\n",
      "118 119\n",
      "AE vs UE; kruskal test result: H:2.63, p:0.105\n",
      "AE performance: 0.73\n",
      "UE performance: 0.63\n",
      "----------------\n",
      "test-859\n",
      "113 122\n",
      "AE vs UE; kruskal test result: H:14.16, p:0.000\n",
      "AE performance: 0.43\n",
      "UE performance: 0.20\n",
      "Alternative AE > UE, pvalue 0.0001 statistic 8469.5000\n",
      "----------------\n",
      "test-388\n",
      "111 120\n",
      "AE vs UE; kruskal test result: H:2.85, p:0.092\n",
      "AE performance: 0.95\n",
      "UE performance: 0.88\n",
      "----------------\n",
      "test-497\n",
      "116 118\n",
      "AE vs UE; kruskal test result: H:4.53, p:0.033\n",
      "AE performance: 0.99\n",
      "UE performance: 0.94\n",
      "----------------\n",
      "test-675\n",
      "116 116\n",
      "AE vs UE; kruskal test result: H:16.18, p:0.000\n",
      "AE performance: 0.03\n",
      "UE performance: 0.21\n",
      "Alternative AE < UE, pvalue 0.0000 statistic 5568.0000\n",
      "----------------\n",
      "avg\n",
      "124 124\n",
      "AE vs UE; kruskal test result: H:1.85, p:0.174\n",
      "AE performance: 0.52\n",
      "UE performance: 0.48\n",
      "----------------\n",
      "--------------------------------\n",
      "acc_execution\n",
      "test-149\n",
      "122 121\n",
      "AE vs UE; kruskal test result: H:0.32, p:0.572\n",
      "AE performance: 0.05\n",
      "UE performance: 0.07\n",
      "----------------\n",
      "test-200\n",
      "118 119\n",
      "AE vs UE; kruskal test result: H:0.03, p:0.852\n",
      "AE performance: 0.74\n",
      "UE performance: 0.75\n",
      "----------------\n",
      "test-859\n",
      "113 122\n",
      "AE vs UE; kruskal test result: H:0.41, p:0.521\n",
      "AE performance: 0.43\n",
      "UE performance: 0.48\n",
      "----------------\n",
      "test-388\n",
      "111 120\n",
      "AE vs UE; kruskal test result: H:2.85, p:0.092\n",
      "AE performance: 0.95\n",
      "UE performance: 0.88\n",
      "----------------\n",
      "test-497\n",
      "116 118\n",
      "AE vs UE; kruskal test result: H:4.53, p:0.033\n",
      "AE performance: 0.99\n",
      "UE performance: 0.94\n",
      "----------------\n",
      "test-675\n",
      "116 116\n",
      "AE vs UE; kruskal test result: H:15.51, p:0.000\n",
      "AE performance: 0.05\n",
      "UE performance: 0.23\n",
      "Alternative AE < UE, pvalue 0.0000 statistic 5510.0000\n",
      "----------------\n",
      "avg\n",
      "124 124\n",
      "AE vs UE; kruskal test result: H:2.49, p:0.115\n",
      "AE performance: 0.52\n",
      "UE performance: 0.55\n",
      "----------------\n",
      "--------------------------------\n",
      "avg 0.53 & 0.46 & 0.50 & 0.51 & - & 0.54 & 0.53 & 0.50 & 0.58 & - & \n",
      "test-149 0.00 & 0.00 & 0.10 & 0.12 & - & 0.00 & 0.00 & 0.10 & 0.14 & - & \n",
      "test-200 0.78 & 0.64 & 0.67 & 0.62 & - & 0.78 & 0.72 & 0.69 & 0.78 & - & \n",
      "test-859 0.44 & 0.12 & 0.42 & 0.29 & AE > UE & 0.44 & 0.42 & 0.42 & 0.53 & - & \n",
      "test-388 0.95 & 0.89 & 0.94 & 0.88 & - & 0.95 & 0.89 & 0.94 & 0.88 & - & \n",
      "test-497 0.98 & 0.91 & 1.00 & 0.98 & - & 0.98 & 0.91 & 1.00 & 0.98 & - & \n",
      "test-675 0.05 & 0.22 & 0.02 & 0.19 & AE < UE & 0.06 & 0.23 & 0.04 & 0.23 & AE < UE & \n"
     ]
    }
   ],
   "source": [
    "str_dict = {}\n",
    "for task_id in task_order + [\"avg\"]:\n",
    "    str_dict[task_id] = \"\"\n",
    "for dimension in [\"acc_strict\", \"acc_execution\"]:\n",
    "    print(dimension)\n",
    "    for task_id in task_order + [\"avg\"]:\n",
    "        print(task_id)\n",
    "        AE_performance = condition_dict[\"AP-AE\"][task_id][dimension] + condition_dict[\"UP-AE\"][task_id][dimension]\n",
    "        UE_performance = condition_dict[\"AP-UE\"][task_id][dimension] + condition_dict[\"UP-UE\"][task_id][dimension]\n",
    "        print(len(AE_performance), len(UE_performance))\n",
    "        statistic, pvalue = kruskal(AE_performance, UE_performance)\n",
    "        print(\"AE vs UE; kruskal test result: H:{:.2f}, p:{:.3f}\".format(statistic, pvalue))\n",
    "        # tp_str = dimension + \"&\" + \"{:.2f} & {:.3f}& \".format(statistic, pvalue)\n",
    "        print(\"AE performance: {:.2f}\".format(np.mean(AE_performance)))\n",
    "        print(\"UE performance: {:.2f}\".format(np.mean(UE_performance)))\n",
    "\n",
    "        if pvalue < 0.05 / 4:\n",
    "            post_hoc_comparison(AE_performance, UE_performance, 'AE', 'UE')\n",
    "        for condition in all_conditions:\n",
    "            data_list_1 = condition_dict[condition][task_id][dimension]\n",
    "            # print(\"{}, Mean: M({}):{:.2f}, SD({}):{:.2f}\".format(len(data_list_1), condition, np.mean(data_list_1), condition, np.std(data_list_1)))\n",
    "            str_dict[task_id] += \"{:.2f} & \".format(np.mean(data_list_1))\n",
    "        if pvalue < 0.05 / 4:\n",
    "            if np.mean(AE_performance) < np.mean(UE_performance):\n",
    "                str_dict[task_id] += \"AE < UE & \"\n",
    "            else:\n",
    "                str_dict[task_id] += \"AE > UE & \"\n",
    "        else:\n",
    "            str_dict[task_id] += \"- & \"\n",
    "        print(\"-\" * 16)\n",
    "    print(\"-\" * 32)\n",
    "for task_id in [\"avg\"] + task_order:\n",
    "    print(task_id, str_dict[task_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
